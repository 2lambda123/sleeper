#
# Copyright 2022 Crown Copyright
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# A string to uniquely identify this deployment. This should be no longer than 20 chars.
# It should be globally unique as it will be used to name AWS resources such as S3 buckets.
sleeper.id=full-example

# A comma separated list of paths containing the table properties files. These can either be paths to
# the properties files themselves or paths to directories which contain the table properties.
sleeper.table.properties=example/full/table.properties,/path/to/my/folder

# The S3 bucket containing the jar files of the Sleeper components.
sleeper.jars.bucket=<the name of the bucket containing your jars, e.g. sleeper-<insert-unique-name-here>-jars

# A comma-separated list of the jars containing application specific iterator code.
# These jars are assumed to be in the bucket given by sleeper.jars.bucket, e.g. if that
# bucket contains two iterator jars called iterator1.jar and iterator2.jar then the
# property should be sleeper.userjars=iterator1.jar,iterator2.jar
sleeper.userjars=

# A file of key-value tags. These will be added to all the resources in this deployment.
sleeper.tags.file=example/full/tags.properties

# A name for a tag to identify the stack that deployed a resource. This will be set for all AWS resources, to the ID of
# the CDK stack that they are deployed under. This can be used to organise the cost explorer for billing.
sleeper.stack.tag.name=DeploymentStack

# Whether to keep the sleeper table bucket, Dynamo tables, query results bucket, etc., when the
# instance is destroyed.
sleeper.retain.infra.after.destroy=true

# The optional stacks to deploy.
sleeper.optional.stacks=CompactionStack,GarbageCollectorStack,IngestStack,PartitionSplittingStack,QueryStack,AthenaStack,EmrBulkImportStack,DashboardStack

# The AWS account number. This is the AWS account that the instance will be deployed to.
sleeper.account=1234567890

# The AWS region to deploy to.
sleeper.region=eu-west-2

# The version of Sleeper to use. This property is used to identify the correct jars in the S3JarsBucket and to
# select the correct tag in the ECR repositories.
sleeper.version=0.13.0-SNAPSHOT

# The id of the VPC to deploy to.
sleeper.vpc=1234567890

# Whether to check that the VPC that the instance is deployed to has an S3 endpoint. If there
# is no S3 endpoint then the NAT costs can be very significant.
sleeper.vpc.endpoint.check=true

# The subnet to deploy ECS tasks to.
sleeper.subnet=subnet-abcdefgh

# The Hadoop filesystem used to connect to S3.
sleeper.filesystem=s3a://

# An email address used by the TopicStack to publish SNS notifications of errors.
sleeper.errors.email=

# The visibility timeout on the queues used in compactions, partition splitting, etc.
sleeper.queue.visibility.timeout.seconds=900

# The length of time in days that CloudWatch logs from lambda functions, ECS containers, etc., are retained.
# See https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-logs-loggroup.html for valid options.
# Use -1 to indicate infinite retention.
sleeper.log.retention.days=30

# Used to set the value of fs.s3a.connection.maximum on the Hadoop configuration. This controls the
# maximum number of http connections to S3. See https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/performance.html
sleeper.s3.max-connections=25

# The version of Fargate to use.
sleeper.fargate.version=1.4.0

# The amount of memory for the lambda that creates ECS tasks to execute compaction and ingest jobs.
sleeper.task.runner.memory=1024

# The timeout in seconds for the lambda that creates ECS tasks to execute compaction jobs and ingest jobs.
# This must be >0 and <= 900.
sleeper.task.runner.timeout.seconds=900

# The namespaces for the metrics used in the metrics stack.
sleeper.metrics.namespace=Sleeper


## The following properties relate to standard ingest.

# The name of the ECR repository for the ingest container. The Docker image from the ingest module should have been
# uploaded to an ECR repository of this name in this account.
sleeper.ingest.repo=<insert-unique-sleeper-id>/ingest

# The maximum number of concurrent ECS tasks to run.
sleeper.ingest.max.concurrent.tasks=200

# The frequency in minutes with which an EventBridge rule runs to trigger a lambda that, if necessary, runs more ECS
# tasks to perform ingest jobs.
sleeper.ingest.task.creation.period.minutes=1

# The frequency, in seconds, with which change message visibility requests are sent to extend the
# visibility of messages on the ingest queue so that they are not processed by other processes.
# This should be less than the value of sleeper.queue.visibility.timeout.seconds.
sleeper.ingest.keepalive.period.seconds=300

# This sets the value of fs.s3a.experimental.input.fadvise on the Hadoop configuration used to read and write
# files to and from S3 in ingest jobs. Changing this value allows you to fine-tune how files are read. Possible
# values are "normal", "sequential" and "random". More information is available here:
# https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/performance.html#fadvise
sleeper.ingest.fs.s3a.experimental.input.fadvise=sequential

# The amount of CPU and memory used by Fargate tasks that perform ingest jobs. Note that only certain combinations
# are valid (see https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-cpu-memory-error.html).
sleeper.ingest.task.cpu=2048
sleeper.ingest.task.memory=4096

# The frequeney in seconds with which ingest tasks refresh their view of the partitions.
# (NB Refreshes only happen once a batch of data has been written so this is a lower bound
# on the refresh frequency.)
sleeper.ingest.partition.refresh.period=120

# The way in which records are held in memory before they are written to a local store.
# Valid values are 'arraylist' and 'arrow'.
# The arraylist method is simpler, but it is slower and requires careful tuning of the number of records in each batch.
# Note that the arrow approach does not currently support schemas containing lists or maps.
sleeper.ingest.record.batch.type=arraylist

# The way in which partition files are written to the main Sleeper store
# Valid values are 'direct' (which writes using the s3a Hadoop file system) and 'async' (which writes locally and then
# copies the completed Parquet file asynchronously into S3).
# The direct method is simpler but the async method should provide better performance when the number of partitions
# is large.
sleeper.ingest.partition.file.writer.type=direct

# The maximum number of records written to local file in an ingest job. (Records are written in sorted order to local
# disk before being uploaded to S3. Increasing this value increases the amount of time before data is visible in the
# system, but increases the number of records written to S3 in a batch, therefore reducing costs.)
# (arraylist-based ingest only)
sleeper.ingest.max.local.records=100000000

# The maximum number of records to read into memory in an ingest job. (Up to sleeper.ingest.memory.max.batch.size
# records are read into memory before being sorted and written to disk. This process is repeated until
# sleeper.ingest.max.local.records records have been written to local files. Then the sorted files and merged and
# the data is written to sorted files in S3.)
# (arraylist-based ingest only)
sleeper.ingest.memory.max.batch.size=1000000

# The number of bytes to allocate to the Arrow working buffer. This buffer is used for sorting and other sundry
# activities.
# Note that this is off-heap memory, which is in addition to the memory assigned to the JVM.
# (arrow-based ingest only) [256MB]
sleeper.ingest.arrow.working.buffer.bytes=268435456

# The number of bytes to allocate to the Arrow batch buffer, which is used to hold the records before they are
# written to local disk. A larger value means that the local disk holds fewer, larger files, which are more efficient
# to merge together during an upload to S3. Larger values may require a larger working buffer.
# Note that this is off-heap memory, which is in addition to the memory assigned to the JVM.
# (arrow-based ingest only) [1GB]
sleeper.ingest.arrow.batch.buffer.bytes=1073741824

# The maximum number of bytes to store on the local disk before uploading to the main Sleeper store. A larger value
# reduces the number of S3 PUTs that are required to upload thle data to S3 and results in fewer files per partition.
# (arrow-based ingest only) [16GB]
sleeper.ingest.arrow.max.local.store.bytes=17179869184

# The number of records to write at once into an Arrow file in the local store. A single Arrow file contains many of
# these micro-batches and so this parameter does not significantly affect the final size of the Arrow file.
# Larger values may require a larger working buffer.
# (arrow-based ingest only) [1K]
sleeper.ingest.arrow.max.single.write.to.file.records=1024

# The name of a bucket that contains files to be ingested via ingest jobs. This bucket should already
# exist, i.e. it will not be created as part of the cdk deployment of this instance of Sleeper. The ingest
# and bulk import stacks will be given read access to this bucket so that they can consume data from it.
sleeper.ingest.source.bucket=


## The following properties relate to bulk import, i.e. ingesting data using Spark jobs running on EMR or EKS.

# In the Dataframe-based Spark bulk import job, the data is normally coalesced into the same number of partitions
# as there are leaf partitions in the table. This can cause problems if there are only a small number of partitions
# in the table. This property sets the minimum number of partitions for this coalesce to happen, e.g. if there
# are fewer than 100 leaf partitions then the coalesce will not happen.
sleeper.bulk.import.min.partitions.coalesce=100

# The class to use to perform the bulk import. The default value below uses Spark Dataframes. There is an
# alternative option that uses RDDs (sleeper.bulkimport.job.runner.rdd.BulkImportJobRDDRunner).
sleeper.bulk.import.class.name=sleeper.bulkimport.job.runner.dataframe.BulkImportJobDataframeRunner

# The compression codec for map status results.
# Used to set spark.shuffle.mapStatus.compression.codec.
sleeper.bulk.import.emr.spark.shuffle.mapStatus.compression.codec=lz4

# If true then speculative execution of tasks will be performed. Used to set spark.speculation.
# See https://spark.apache.org/docs/latest/configuration.html.
sleeper.bulk.import.emr.spark.speculation=false

# Fraction of tasks which must be complete before speculation is enabled for a particular stage.
# Used to set spark.speculation.quantile.
# See https://spark.apache.org/docs/latest/configuration.html.
sleeper.bulk.import.spark.speculation.quantile=0.75

# The name of the bucket created by bulk import stacks for storing of logs from EMR-based bulk import
# stacks, and for storing the JSON files defining the jobs.
sleeper.bulk.import.emr.bucket=

# The amount of memory allocated to a Spark executor. Used to set spark.executor.memory.
# See https://spark.apache.org/docs/latest/configuration.html.
sleeper.bulk.import.emr.spark.executor.memory=16g

# The amount of memory allocated to the Spark driver. Used to set spark.driver.memory.
# See https://spark.apache.org/docs/latest/configuration.html.
sleeper.bulk.import.emr.spark.driver.memory=16g

# The number of executors. Used to set spark.executor.instances.
# See https://spark.apache.org/docs/latest/configuration.html.
sleeper.bulk.import.emr.spark.executor.instances=29

# The memory overhead for an executor. Used to set spark.yarn.executor.memoryOverhead.
# See https://spark.apache.org/docs/latest/configuration.html.
sleeper.bulk.import.emr.spark.yarn.executor.memory.overhead=2g

# The memory overhead for the driver. Used to set spark.yarn.driver.memoryOverhead.
# See https://spark.apache.org/docs/latest/configuration.html.
sleeper.bulk.import.emr.spark.yarn.driver.memory.overhead=2g

# The default parallelism for Spark job. Used to set spark.default.parallelism.
# See https://spark.apache.org/docs/latest/configuration.html.
sleeper.bulk.import.emr.spark.default.parallelism=290

# The number of partitions used in a Spark SQL/dataframe shuffle operation. Used to set spark.sql.shuffle.partitions.
# See https://spark.apache.org/docs/latest/configuration.html.
sleeper.bulk.import.emr.spark.sql.shuffle.partitions=290


# (Non-persistent or persistent EMR mode only) An EC2 keypair to use for the EC2 instances. Specifying this will allow you to SSH to the nodes
# in the cluster while it's running.
sleeper.bulk.import.emr.keypair.name=my-key

# (Non-persistent or persistent EMR mode only) Specifying this security group causes the group to be added to the EMR master's
# list of security groups.
sleeper.bulk.import.emr.master.additional.security.group=

# (Non-persistent or persistent EMR mode only) The number of cores used by an executor. Used to set spark.executor.cores.
# See https://spark.apache.org/docs/latest/configuration.html.
sleeper.bulk.import.emr.spark.executor.cores=5

# (Non-persistent or persistent EMR mode only) The number of cores used by the driver. Used to set spark.driver.cores.
# See https://spark.apache.org/docs/latest/configuration.html.
sleeper.bulk.import.emr.spark.driver.cores=5

# (Non-persistent or persistent EMR mode only) The default timeout for network interactions in Spark. Used to set spark.network.timeout.
# See https://spark.apache.org/docs/latest/configuration.html.
sleeper.bulk.import.emr.spark.network.timeout=800s

# (Non-persistent or persistent EMR mode only) The interval between heartbeats from executors to the driver.
# Used to set spark.executor.heartbeatInterval.
# See https://spark.apache.org/docs/latest/configuration.html.
sleeper.bulk.import.emr.spark.executor.heartbeat.interval=60s

# (Non-persistent or persistent EMR mode only) Whether Spark should use dynamic allocation to scale resources up and down.
# Used to set spark.dynamicAllocation.enabled.
# See https://spark.apache.org/docs/latest/configuration.html.
sleeper.bulk.import.emr.spark.dynamic.allocation.enabled=false

# (Non-persistent or persistent EMR mode only) The fraction of heap space used for execution and storage.
# Used to set spark.memory.fraction.
# See https://spark.apache.org/docs/latest/configuration.html.
sleeper.bulk.import.emr.spark.memory.fraction=0.80

# (Non-persistent or persistent EMR mode only) The amount of storage memory immune to eviction, expressed as a fraction of
# the heap space used for execution and storage. Used to set spark.memory.storageFraction.
# See https://spark.apache.org/docs/latest/configuration.html.
sleeper.bulk.import.emr.spark.memory.storage.fraction=0.30

# (Non-persistent or persistent EMR mode only) JVM options passed to the executors.
# Used to set spark.executor.extraJavaOptions.
# See https://spark.apache.org/docs/latest/configuration.html.
sleeper.bulk.import.emr.spark.executor.extra.java.options=-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'

# (Non-persistent or persistent EMR mode only) JVM options passed to the driver.
# Used to set spark.driver.extraJavaOptions.
# See https://spark.apache.org/docs/latest/configuration.html.
sleeper.bulk.import.emr.spark.driver.extra.java.options=-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'

# (Non-persistent or persistent EMR mode only) The maximum number of executor failures before YARN can fail the application.
# Used to set spark.yarn.scheduler.reporterThread.maxFailures.
# See https://aws.amazon.com/blogs/big-data/best-practices-for-successfully-managing-memory-for-apache-spark-applications-on-amazon-emr/.
sleeper.bulk.import.emr.spark.yarn.scheduler.reporter.thread.max.failures=5

# (Non-persistent or persistent EMR mode only) The storage to use for temporary caching.
# Used to set spark.storage.level.
# See https://aws.amazon.com/blogs/big-data/best-practices-for-successfully-managing-memory-for-apache-spark-applications-on-amazon-emr/.
sleeper.bulk.import.emr.spark.storage.level=MEMORY_AND_DISK_SER

# (Non-persistent or persistent EMR mode only) Whether to compress serialized RDD partitions.
# Used to set spark.rdd.compress.
# See https://spark.apache.org/docs/latest/configuration.html.
sleeper.bulk.import.emr.spark.rdd.compress=true

# (Non-persistent or persistent EMR mode only) Whether to compress map output files.
# Used to set spark.shuffle.compress.
# See https://spark.apache.org/docs/latest/configuration.html.
sleeper.bulk.import.emr.spark.shuffle.compress=true

# (Non-persistent or persistent EMR mode only) Whether to compress data spilled during shuffles.
# Used to set spark.shuffle.spill.compress.
# See https://spark.apache.org/docs/latest/configuration.html.
sleeper.bulk.import.emr.spark.shuffle.spill.compress=true


# (Non-persistent EMR mode only) The default EMR release label to be used when creating an EMR cluster for bulk importing data
# using Spark running on EMR. This default can be overridden by a table property or by a property in the
# bulk import job specification.
sleeper.default.bulk.import.emr.release.label=emr-6.8.0

# (Non-persistent EMR mode only) The default EC2 instance type to be used for the master node of the EMR cluster. This default can
# be overridden by a table property or by a property in the bulk import job specification.
sleeper.default.bulk.import.emr.master.instance.type=m5.xlarge

# (Non-persistent EMR mode only) The default EC2 instance type to be used for the executor nodes of the EMR cluster. This default can
# be overridden by a table property or by a property in the bulk import job specification.
sleeper.default.bulk.import.emr.executor.instance.type=m5.4xlarge

# (Non-persistent EMR mode only) The default initial number of EC2 instances to be used as executors in the EMR cluster. This default can
# be overridden by a table property or by a property in the bulk import job specification.
sleeper.default.bulk.import.emr.executor.initial.instances=2

# (Non-persistent EMR mode only) The default maximum number of EC2 instances to be used as executors in the EMR cluster. This default can
# be overridden by a table property or by a property in the bulk import job specification.
sleeper.default.bulk.import.emr.executor.max.instances=10


# (Persistent EMR mode only) The EMR release used to create the persistent EMR cluster.
sleeper.bulk.import.persistent.emr.release.label=emr-6.8.0

# (Persistent EMR mode only) The EC2 instance type used for the master of the persistent EMR cluster.
sleeper.bulk.import.persistent.emr.master.instance.type=m5.xlarge

# (Persistent EMR mode only) The EC2 instance type used for the executor nodes of the persistent EMR cluster.
sleeper.bulk.import.persistent.emr.core.instance.type=m5.4xlarge

# (Persistent EMR mode only) Whether the persistent EMR cluster should use managed scaling or not.
sleeper.bulk.import.persistent.emr.use.managed.scaling=true

# (Persistent EMR mode only) The minimum number of instances in the persistent EMR cluster. If managed
# scaling is not used then the cluster will be of fixed size, with number of instances equal to this
# value.
sleeper.bulk.import.persistent.emr.min.instances=1

# (Persistent EMR mode only) The maximum number of instances in the persistent EMR cluster. This value
# is only used if managed scaling is not used.
sleeper.bulk.import.persistent.emr.max.instances=10

# (EKS mode only) The name of the ECS repository where the Docker image for the bulk import container is stored.
sleeper.bulk.import.eks.repo=<insert-unique-sleeper-id>//bulk-import-runner


## The following properties relate to the splitting of partitions.

# The frequency in minutes with which the lambda that finds partitions that need splitting runs.
sleeper.partition.splitting.period.minutes=60

# When a partition needs splitting, a partition splitting job is created. This reads in the sketch files
# associated to the files in the partition in order to identify the median. This parameter controls the
# maximum number of files that are read in.
sleeper.partition.splitting.files.maximum=50

# The amount of memory in MB for the lambda function used to identify partitions that need to be split.
sleeper.partition.splitting.finder.memory=2048

# The timeout in seconds for the lambda function used to identify partitions that need to be split.
sleeper.partition.splitting.finder.timeout.seconds=900

# The memory for the lambda function used to split partitions.
sleeper.partition.splitting.memory=2048

# The timeout in seconds for the lambda function used to split partitions.
sleeper.partition.splitting.timeout.seconds=900

# This is the default value of the partition splitting threshold. Partitions with more than the following
# number of records in will be split. This value can be overridden on a per-table basis.
sleeper.default.partition.splitting.threshold=1000000000


## The following properties relate to garbage collection.

# The frequency in minutes with which the garbage collector lambda is run.
sleeper.gc.period.minutes=15

# The memory in MB for the lambda function used to perform garbage collection.
sleeper.gc.memory=1024

# The size of the batch of files ready for garbage collection requested from the State Store.
sleeper.gc.batch.size=2000

# A file will not be deleted until this number of seconds have passed after it has been marked as ready for
# garbage collection. The reason for not deleting files immediately after they have been marked as ready for
# garbage collection is that they may still be in use by queries. This property can be overridden on a per-table
# basis.
sleeper.default.gc.delay.seconds=600


## The following properties relate to compactions.

# The name of the repository for the compaction container. The Docker image from the compaction-job-execution module
# should have been uploaded to an ECR repository of this name in this account.
sleeper.compaction.repo=<insert-unique-sleeper-id>/compaction-job-execution

# The visibility timeout for the queue of compaction jobs.
sleeper.compaction.queue.visibility.timeout.seconds=900

# The frequency, in seconds, with which change message visibility requests are sent to extend the
# visibility of messages on the compaction job queue so that they are not processed by other processes.
# This should be less than the value of sleeper.queue.visibility.timeout.seconds.
sleeper.compaction.keepalive.period.seconds=300

# The rate at which the compaction job creation lambda runs (in minutes, must be >=1).
sleeper.compaction.job.creation.period.minutes=1

# The amount of memory for the lambda that creates compaction jobs.
sleeper.compaction.job.creation.memory=1024

# The timeout for the lambda that creates compaction jobs in seconds.
sleeper.compaction.job.creation.timeout.seconds=900

# The maximum number of concurrent compaction tasks to run.
sleeper.compaction.max.concurrent.tasks=300

# The rate at which a check to see if compaction ECS tasks need to be created is made (in minutes, must be >= 1).
sleeper.compaction.task.creation.period.minutes=1

# The cpu and memory for a compaction task.
# See https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-cpu-memory-error.html for valid options.
sleeper.compaction.task.cpu=2048
sleeper.compaction.task.memory=4096

# Flag to enable/disable storage of tracking information for compaction jobs and tasks.
sleeper.compaction.status.store.enabled=true

# The name of the class that defines how compaction jobs should be created.
# This should implement sleeper.compaction.strategy.CompactionStrategy. The value of this property is the
# default value which can be overridden on a per-table basis.
sleeper.default.compaction.strategy.class=sleeper.compaction.strategy.impl.SizeRatioCompactionStrategy

# The minimum number of files to read in a compaction job. Note that the state store
# must support atomic updates for this many files. For the DynamoDBStateStore this
# is 11. It can be overridden on a per-table basis.
# (NB This does not apply to splitting jobs which will run even if there is only 1 file.)
# This is a default value and will be used if not specified in the table.properties file
sleeper.default.compaction.files.batch.size=11

# Used by the SizeRatioCompactionStrategy to decide if a group of files should be compacted.
# If the file sizes are s_1, ..., s_n then the files are compacted if s_1 + ... + s_{n-1} >= ratio * s_n.
# It can be overridden on a per-table basis.
sleeper.default.table.compaction.strategy.sizeratio.ratio=3

# Used by the SizeRatioCompactionStrategy to control the maximum number of jobs that can be running
# concurrently per partition. It can be overridden on a per-table basis.
sleeper.default.table.compaction.strategy.sizeratio.max.concurrent.jobs.per.partition=100000


## The following properties relate to queries.

# The maximum number of simultaneous connections to S3 from a single query runner. This is separated
# from the main one as it's common for a query runner to need to open more files at once.
sleeper.query.s3.max-connections=1024

# The amount of memory in MB for the lambda that executes queries.
sleeper.query.processor.memory=2048

# The timeout for the lambda that executes queries in seconds.
sleeper.query.processor.timeout.seconds=900

# The frequency with which the query processing lambda refreshes its knowledge of the system state
# (i.e. the partitions and the mapping from partition to files), in seconds.
sleeper.query.processor.state.refresh.period.seconds=60

# The maximum number of records to include in a batch of query results send to
# the results queue from the query processing lambda.
sleeper.query.processor.results.batch.size=2000

# The size of the thread pool for retrieving records in a query processing lambda.
sleeper.query.processor.record.retrieval.threads=10

# This value is used to set the time-to-live on the tracking of the queries in the DynamoDB-based query tracker.
sleeper.query.tracker.ttl.days=1

# The length of time the results of queries remain in the query results bucket before being deleted.
sleeper.query.results.bucket.expiry.days=7

# The default value of the rowgroup size used when the results of queries are written to Parquet files. The
# value given below is 8MiB. This value can be overridden using the query config.
sleeper.default.query.results.rowgroup.size=8388608

# The default value of the page size used when the results of queries are written to Parquet files. The
# value given below is 128KiB. This value can be overridden using the query config.
sleeper.default.query.results.page.size=131072


## The following properties relate to the dashboard.

# The period in minutes used in the dashboard.
sleeper.dashboard.time.window.minutes=5


## The following properties relate to logging.

# Logging Levels. These set the logging levels for different classes throughout the app. The list of allowed values
# are (from most verbose to least): TRACE, DEBUG, INFO (default), WARN, ERROR, FATAL. If you want more control than
# this, you can edit the log4j.properties file in core/src/main/resources

# For logging Sleeper classes. This does not apply to the MetricsLogger which is always set to INFO.
sleeper.logging.level=INFO

# For Apache logs that are not Parquet.
sleeper.logging.apache.level=INFO

# For Parquet logs.
sleeper.logging.parquet.level=WARN

# For AWS logs.
sleeper.logging.aws.level=INFO

# For everything else.
sleeper.logging.root.level=INFO


## The following properties relate to the integration with Athena.

# The number of days before objects in the spill bucket are deleted.
sleeper.athena.spill.bucket.ageoff.days=1

# The fully qualified composite classes to deploy. These are the classes that interact with Athena.
# You can choose to remove one if you don't need them. Both are deployed by default.
sleeper.athena.handler.classes=sleeper.athena.composite.SimpleCompositeHandler,sleeper.athena.composite.IteratorApplyingHandler

# The amount of memory (GB) the athena composite handler has
sleeper.athena.handler.memory=4096

# The timeout in seconds for the athena composite handler
sleeper.athena.handler.timeout.seconds=900


## The following properties are default values.

# The readahead range set on the Hadoop configuration when reading Parquet files in a query
# (see https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/index.html).
sleeper.default.fs.s3a.readahead.range=64K

# The size of the row group in the Parquet files (default is 8MiB).
sleeper.default.rowgroup.size=8388608

# The size of the pages in the Parquet files (default is 128KiB).
sleeper.default.page.size=131072

# The compression codec to use in the Parquet files
sleeper.default.compression.codec=ZSTD

# This specifies whether point in time recovery is turned on for DynamoDB tables. This default can
# be overridden by a table property.
sleeper.default.table.dynamo.pointintimerecovery=false

# This specifies whether queries and scans against DynamoDB tables used in the DynamoDB state store
# are strongly consistent. This default can be overriden by a table property.
sleeper.default.table.dynamo.strongly.consistent.reads=false
